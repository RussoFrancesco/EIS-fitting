{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fcbad5bb",
      "metadata": {
        "id": "fcbad5bb"
      },
      "source": [
        "## Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44275ab6",
      "metadata": {},
      "outputs": [],
      "source": [
        "SEQ_LEN_IN = 128\n",
        "SEQ_LEN_OUT = 10\n",
        "NUM_FEATURES_IN = 121\n",
        "NUM_FEATURES_OUT = 121\n",
        "TRAIN_SPLIT = 0.7\n",
        "VAL_SPLIT = 0.15\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 1000\n",
        "PATIENCE = 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "935d29eb-4bd1-4476-ad4e-c1717adb7f04",
      "metadata": {
        "id": "935d29eb-4bd1-4476-ad4e-c1717adb7f04"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from joblib import dump, load\n",
        "import keras\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90b20e1d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_sequences_for_seq2seq(data, seq_len_in, seq_len_out, train_size=0.7, val_size=0.15):\n",
        "    X_enc, y = [], []\n",
        "\n",
        "    for i in range(len(data) - (seq_len_in + seq_len_out) + 1):\n",
        "        X_enc.append(data[i : i + seq_len_in])\n",
        "        y.append(data[i + seq_len_in : i + seq_len_in + seq_len_out])\n",
        "\n",
        "    X_enc = np.array(X_enc)\n",
        "    y = np.array(y)\n",
        "\n",
        "    X_dec = y\n",
        "\n",
        "    total_samples = len(X_enc)\n",
        "    train_end = int(total_samples * train_size)\n",
        "    val_end = int(total_samples * (train_size + val_size))\n",
        "\n",
        "    X_train_enc, y_train = X_enc[:train_end], y[:train_end]\n",
        "    X_val_enc, y_val = X_enc[train_end:val_end], y[train_end:val_end]\n",
        "    X_test_enc, y_test = X_enc[val_end:], y[val_end:]\n",
        "\n",
        "    X_train_dec = X_dec[:train_end]\n",
        "    X_val_dec = X_dec[train_end:val_end]\n",
        "    X_test_dec = X_dec[val_end:]\n",
        "\n",
        "    return (\n",
        "        X_train_enc, X_val_enc, X_test_enc,\n",
        "        X_train_dec, X_val_dec, X_test_dec,\n",
        "        y_train, y_val, y_test\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "100e12c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "BLOCK_SIZE = 128\n",
        "\n",
        "N_BINS_SOH = 5\n",
        "\n",
        "print(\"--- Fase 1: Caricamento e preparazione dei dati ---\")\n",
        "files = glob.glob(\"cell*.csv\")\n",
        "dataframes = []\n",
        "for path in files:\n",
        "    cell = os.path.basename(path).split('.')[0]\n",
        "    df = pd.read_csv(path)\n",
        "    df.drop(columns=[col for col in df.columns if col.startswith('f_')], inplace=True)\n",
        "    df['Cell'] = cell\n",
        "    dataframes.append(df)\n",
        "df_all = pd.concat(dataframes, ignore_index=True)\n",
        "feature_cols = [col for col in df_all.columns if col != 'Cell']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(f\"\\n--- Fase 2: Creazione di blocchi (dimensione: {BLOCK_SIZE} cicli) ---\")\n",
        "\n",
        "X_enc_blocks, X_dec_blocks, y_blocks = [], [], []\n",
        "cell_id_per_block = []\n",
        "soh_avg_per_block = []\n",
        "\n",
        "scalers_per_cell = {}\n",
        "\n",
        "def create_all_sequences(data, seq_len_in, seq_len_out):\n",
        "    X_enc, y = [], []\n",
        "    for i in range(len(data) - (seq_len_in + seq_len_out) + 1):\n",
        "        X_enc.append(data[i : i + seq_len_in])\n",
        "        y.append(data[i + seq_len_in : i + seq_len_in + seq_len_out])\n",
        "    \n",
        "    if not X_enc:\n",
        "        return None, None, None\n",
        "        \n",
        "    X_enc, y = np.array(X_enc), np.array(y)\n",
        "    X_dec = y\n",
        "    return X_enc, X_dec, y\n",
        "\n",
        "for cell_id, group in df_all.groupby('Cell'):\n",
        "    print(f\"Processando cella: {cell_id}\")\n",
        "    scaler = StandardScaler()\n",
        "    data_to_scale = group[feature_cols].values\n",
        "    \n",
        "    if len(data_to_scale) < SEQ_LEN_IN + SEQ_LEN_OUT:\n",
        "        print(f\"  -> Dati insufficienti per {cell_id}. Saltato.\")\n",
        "        continue\n",
        "        \n",
        "    data_cell_scaled = scaler.fit_transform(data_to_scale)\n",
        "    scalers_per_cell[cell_id] = scaler\n",
        "\n",
        "    X_enc_cell, X_dec_cell, y_cell = create_all_sequences(data_cell_scaled, SEQ_LEN_IN, SEQ_LEN_OUT)\n",
        "    if X_enc_cell is None:\n",
        "        continue\n",
        "\n",
        "    num_sequences_in_cell = len(X_enc_cell)\n",
        "    for i in range(0, num_sequences_in_cell, BLOCK_SIZE):\n",
        "        start_idx = i\n",
        "        end_idx = i + BLOCK_SIZE\n",
        "        \n",
        "        if end_idx > num_sequences_in_cell:\n",
        "            continue\n",
        "\n",
        "        X_enc_blocks.append(X_enc_cell[start_idx:end_idx])\n",
        "        X_dec_blocks.append(X_dec_cell[start_idx:end_idx])\n",
        "        y_blocks.append(y_cell[start_idx:end_idx])\n",
        "        \n",
        "        cell_id_per_block.append(cell_id)\n",
        "        soh_avg_per_block.append(np.mean(y_cell[start_idx:end_idx, :, -1]))\n",
        "\n",
        "dump(scalers_per_cell, 'scalers_per_cell.joblib')\n",
        "print(f\"Creati {len(X_enc_blocks)} blocchi totali.\")\n",
        "\n",
        "print(\"\\n--- Fase 3: Esecuzione dello split stratificato sui blocchi ---\")\n",
        "\n",
        "block_indices = np.arange(len(X_enc_blocks))\n",
        "\n",
        "soh_bins = pd.cut(soh_avg_per_block, bins=N_BINS_SOH, labels=False)\n",
        "\n",
        "train_val_indices, test_indices = train_test_split(\n",
        "    block_indices,\n",
        "    test_size=(1 - TRAIN_SPLIT - VAL_SPLIT), \n",
        "    stratify=soh_bins,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "soh_bins_train_val = soh_bins[train_val_indices]\n",
        "train_indices, val_indices = train_test_split(\n",
        "    train_val_indices,\n",
        "    test_size=VAL_SPLIT / (TRAIN_SPLIT + VAL_SPLIT),\n",
        "    stratify=soh_bins_train_val,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"\\n--- Fase 4: Costruzione dei set di training, validazione e test ---\")\n",
        "\n",
        "def build_dataset_from_blocks(indices, x_enc_b, x_dec_b, y_b, cell_ids_b):\n",
        "    if len(indices) == 0:\n",
        "        return np.array([]), np.array([]), np.array([]), np.array([])\n",
        "    \n",
        "    X_enc = np.concatenate([x_enc_b[i] for i in indices], axis=0)\n",
        "    X_dec = np.concatenate([x_dec_b[i] for i in indices], axis=0)\n",
        "    y = np.concatenate([y_b[i] for i in indices], axis=0)\n",
        "    \n",
        "    cell_ids_list = []\n",
        "    for i in indices:\n",
        "        cell_ids_list.extend([cell_ids_b[i]] * len(y_b[i]))\n",
        "    \n",
        "    return X_enc, X_dec, y, np.array(cell_ids_list)\n",
        "\n",
        "X_train_enc, X_train_dec, y_train, cell_ids_train = build_dataset_from_blocks(train_indices, X_enc_blocks, X_dec_blocks, y_blocks, cell_id_per_block)\n",
        "X_val_enc, X_val_dec, y_val, cell_ids_val = build_dataset_from_blocks(val_indices, X_enc_blocks, X_dec_blocks, y_blocks, cell_id_per_block)\n",
        "X_test_enc, X_test_dec, y_test, cell_ids_test = build_dataset_from_blocks(test_indices, X_enc_blocks, X_dec_blocks, y_blocks, cell_id_per_block)\n",
        "\n",
        "print(\"\\n--- Fase 5: Mescolamento delle sequenze all'interno dei set ---\\n\")\n",
        "\n",
        "def shuffle_sets(*arrays):\n",
        "    if not all(len(arr) > 0 for arr in arrays):\n",
        "        return arrays\n",
        "    idx = np.random.permutation(len(arrays[0]))\n",
        "    return [arr[idx] for arr in arrays]\n",
        "\n",
        "X_train_enc, X_train_dec, y_train, cell_ids_train = shuffle_sets(X_train_enc, X_train_dec, y_train, cell_ids_train)\n",
        "X_val_enc, X_val_dec, y_val, cell_ids_val = shuffle_sets(X_val_enc, X_val_dec, y_val, cell_ids_val)\n",
        "#X_test_enc, X_test_dec, y_test, cell_ids_test = shuffle_sets(X_test_enc, X_test_dec, y_test, cell_ids_test)\n",
        "\n",
        "print(f\"Forma finale X_train_enc: {X_train_enc.shape}\")\n",
        "print(f\"Forma finale y_train: {y_train.shape}\")\n",
        "print(f\"Forma finale X_val_enc: {X_val_enc.shape}\")\n",
        "print(f\"Forma finale y_val: {y_val.shape}\")\n",
        "print(f\"Forma X_test_enc (prima dell'ordinamento): {X_test_enc.shape}\")\n",
        "print(f\"Forma y_test (prima dell'ordinamento): {y_test.shape}\")\n",
        "print(f\"Forma cell_ids_test (prima dell'ordinamento): {cell_ids_test.shape}\")\n",
        "\n",
        "\n",
        "print(\"Test set ordinato con successo.\")\n",
        "print(\"\\nVerifica dell'ordinamento:\")\n",
        "print(f\"Primi 10 ID cella nel test set ordinato: {cell_ids_test[:10]}\")\n",
        "print(f\"Ultimi 10 ID cella nel test set ordinato: {cell_ids_test[-10:]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a1bbe8a",
      "metadata": {
        "id": "3a1bbe8a"
      },
      "outputs": [],
      "source": [
        "def complex_mse_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calcola l'errore quadratico medio sul piano complesso.\n",
        "    Questo forza il modello a imparare la relazione tra parte reale e immaginaria.\n",
        "    \"\"\"\n",
        "\n",
        "    r_idx = tf.constant(list(range(0, 118, 2)))\n",
        "    i_idx = tf.constant(list(range(1, 118, 2)))\n",
        "\n",
        "    # tf.gather permette di selezionare colonne specifiche da un tensore\n",
        "    re_true = tf.gather(y_true, r_idx, axis=-1)\n",
        "    im_true = tf.gather(y_true, i_idx, axis=-1)\n",
        "\n",
        "    re_pred = tf.gather(y_pred, r_idx, axis=-1)\n",
        "    im_pred = tf.gather(y_pred, i_idx, axis=-1)\n",
        "\n",
        "    z_true = tf.complex(re_true, im_true)\n",
        "    z_pred = tf.complex(re_pred, im_pred)\n",
        "\n",
        "    error = tf.abs(z_true - z_pred)\n",
        "    complex_mse = tf.reduce_mean(tf.square(error))\n",
        "\n",
        "    return complex_mse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e5071e3",
      "metadata": {
        "id": "5e5071e3"
      },
      "outputs": [],
      "source": [
        "def combined_loss(y_true, y_pred):\n",
        "    # Pesi per le diverse componenti della loss\n",
        "    alpha = 10.0  # Peso per la loss complessa\n",
        "    beta = 1.0   # Peso per la loss sulla temperatura\n",
        "    gamma = 1.0  # Peso per la loss su SOH\n",
        "\n",
        "    complex_loss = complex_mse_loss(y_true, y_pred)\n",
        "\n",
        "    #temp_true = y_true[:, :, -2]\n",
        "    soh_true = y_true[:, :, -1]\n",
        "    #temp_pred = y_pred[:, :, -2]\n",
        "    soh_pred = y_pred[:, :, -1]\n",
        "\n",
        "    #temp_mse = tf.reduce_mean(tf.square(temp_true - temp_pred))\n",
        "    soh_mse = tf.reduce_mean(tf.square(soh_true - soh_pred))\n",
        "\n",
        "    total_loss = (alpha * complex_loss) + (gamma * soh_mse)\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41a8d97d",
      "metadata": {
        "id": "41a8d97d"
      },
      "outputs": [],
      "source": [
        "@keras.saving.register_keras_serializable()\n",
        "class EncoderDecoderTransformer(tf.keras.Model):\n",
        "    def __init__(self, \n",
        "                 seq_len_in, seq_len_out, \n",
        "                 num_features_in, num_features_out,\n",
        "                 d_model=64, num_heads=4, ff_dim=128,\n",
        "                 num_encoder_layers=4, num_decoder_layers=4,\n",
        "                 dropout=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # --- Parametri principali ---\n",
        "        self.seq_len_in = seq_len_in\n",
        "        self.seq_len_out = seq_len_out\n",
        "        self.num_features_in = num_features_in\n",
        "        self.num_features_out = num_features_out\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.ff_dim = ff_dim\n",
        "        self.num_encoder_layers = num_encoder_layers\n",
        "        self.num_decoder_layers = num_decoder_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # --- ENCODER ---\n",
        "        self.encoder_input_proj = layers.Dense(d_model)\n",
        "        # positional embedding fino a seq_len_in (128), ma funziona anche se <128\n",
        "        self.encoder_pos_embedding = layers.Embedding(input_dim=seq_len_in, output_dim=d_model)\n",
        "\n",
        "        self.encoder_layers = [\n",
        "            (\n",
        "                layers.LayerNormalization(epsilon=1e-6),\n",
        "                layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model, dropout=dropout),\n",
        "                layers.Dropout(dropout),\n",
        "                layers.LayerNormalization(epsilon=1e-6),\n",
        "                layers.Dense(ff_dim, activation='relu'),\n",
        "                layers.Dense(d_model),\n",
        "                layers.Dropout(dropout)\n",
        "            ) for _ in range(num_encoder_layers)\n",
        "        ]\n",
        "\n",
        "        # --- DECODER ---\n",
        "        self.decoder_input_proj = layers.Dense(d_model)\n",
        "        # positional embedding fino a seq_len_out (10)\n",
        "        self.decoder_pos_embedding = layers.Embedding(input_dim=seq_len_out, output_dim=d_model)\n",
        "\n",
        "        self.decoder_layers = [\n",
        "            (\n",
        "                layers.LayerNormalization(epsilon=1e-6),\n",
        "                layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model, dropout=dropout),\n",
        "                layers.Dropout(dropout),\n",
        "                layers.LayerNormalization(epsilon=1e-6),\n",
        "                layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model, dropout=dropout),\n",
        "                layers.Dropout(dropout),\n",
        "                layers.LayerNormalization(epsilon=1e-6),\n",
        "                layers.Dense(ff_dim, activation='relu'),\n",
        "                layers.Dense(d_model),\n",
        "                layers.Dropout(dropout)\n",
        "            ) for _ in range(num_decoder_layers)\n",
        "        ]\n",
        "\n",
        "        self.output_proj = layers.Dense(num_features_out)\n",
        "\n",
        "    # ---- MASCHERA CAUSALE DINAMICA ----\n",
        "    @staticmethod\n",
        "    def get_causal_attention_mask(x):\n",
        "        \"\"\"\n",
        "        Crea una maschera causale (triangolare inferiore) adattata alla lunghezza runtime.\n",
        "        \"\"\"\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        i = tf.range(seq_len)[:, None]\n",
        "        j = tf.range(seq_len)\n",
        "        mask = tf.cast(i >= j, dtype=tf.float32)\n",
        "        return mask[None, :, :]  # (1, seq_len, seq_len)\n",
        "\n",
        "    # ---- FORWARD PASS ----\n",
        "    def call(self, inputs, training=False):\n",
        "        encoder_input, decoder_input = inputs\n",
        "\n",
        "        # === ENCODER ===\n",
        "        encoder_positions = tf.range(start=0, limit=tf.shape(encoder_input)[1], delta=1)\n",
        "        x = self.encoder_input_proj(encoder_input)\n",
        "        x += self.encoder_pos_embedding(encoder_positions)\n",
        "\n",
        "        for norm1, mha, drop1, norm2, ff1, ff2, drop2 in self.encoder_layers:\n",
        "            attn_output = mha(query=norm1(x), value=norm1(x), key=norm1(x), training=training)\n",
        "            x = x + drop1(attn_output, training=training)\n",
        "            ffn_output = ff2(ff1(norm2(x)))\n",
        "            x = x + drop2(ffn_output, training=training)\n",
        "        encoder_output = x\n",
        "\n",
        "        # === DECODER ===\n",
        "        decoder_positions = tf.range(start=0, limit=tf.shape(decoder_input)[1], delta=1)\n",
        "        y = self.decoder_input_proj(decoder_input)\n",
        "        y += self.decoder_pos_embedding(decoder_positions)\n",
        "\n",
        "        causal_mask = self.get_causal_attention_mask(y)\n",
        "\n",
        "        for norm1, self_mha, drop1, norm2, cross_mha, drop2, norm3, ff1, ff2, drop3 in self.decoder_layers:\n",
        "            self_attn_output = self_mha(\n",
        "                query=norm1(y), value=norm1(y), key=norm1(y),\n",
        "                attention_mask=causal_mask, training=training\n",
        "            )\n",
        "            y = y + drop1(self_attn_output, training=training)\n",
        "\n",
        "            cross_attn_output = cross_mha(\n",
        "                query=norm2(y), value=encoder_output, key=encoder_output, training=training\n",
        "            )\n",
        "            y = y + drop2(cross_attn_output, training=training)\n",
        "\n",
        "            ffn_output = ff2(ff1(norm3(y)))\n",
        "            y = y + drop3(ffn_output, training=training)\n",
        "\n",
        "        return self.output_proj(y)\n",
        "\n",
        "    # ---- SERIALIZZAZIONE ----\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            \"seq_len_in\": self.seq_len_in,\n",
        "            \"seq_len_out\": self.seq_len_out,\n",
        "            \"num_features_in\": self.num_features_in,\n",
        "            \"num_features_out\": self.num_features_out,\n",
        "            \"d_model\": self.d_model,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"ff_dim\": self.ff_dim,\n",
        "            \"num_encoder_layers\": self.num_encoder_layers,\n",
        "            \"num_decoder_layers\": self.num_decoder_layers,\n",
        "            \"dropout\": self.dropout,\n",
        "        }\n",
        "        base_config = super().get_config()\n",
        "        base_config.update(config)\n",
        "        return base_config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fe9d287",
      "metadata": {
        "id": "6fe9d287"
      },
      "outputs": [],
      "source": [
        "def print_eis_predictions(model, X_test_enc, X_test_dec, y_true_norm, scalers, cell_ids_test, index=0):\n",
        "    \"\"\"\n",
        "    Esegue la predizione per un singolo campione di test e plotta la curva di Nyquist\n",
        "    REALE vs PREDETTA per ogni timestep futuro, usando lo scaler corretto per la cella.\n",
        "\n",
        "    Args:\n",
        "        model (tf.keras.Model): Il modello addestrato.\n",
        "        X_test_enc (np.ndarray): Dati di input per l'encoder del set di test.\n",
        "        X_test_dec (np.ndarray): Dati di input per il decoder del set di test.\n",
        "        y_true_norm (np.ndarray): Dati di target reali (normalizzati) del set di test.\n",
        "        scalers (dict): Dizionario di scaler per cella.\n",
        "        cell_ids_test (np.ndarray): Array con gli ID della cella per ogni campione di test.\n",
        "        index (int): Lâ€™indice del campione da visualizzare nel set di test.\n",
        "    \"\"\"\n",
        "    if index >= len(X_test_enc):\n",
        "        print(f\"Errore: l'indice {index} Ã¨ fuori dai limiti. Il test set ha {len(X_test_enc)} campioni.\")\n",
        "        return\n",
        "\n",
        "    sample_X_enc = np.expand_dims(X_test_enc[index], axis=0)\n",
        "    sample_X_dec = np.expand_dims(X_test_dec[index], axis=0)\n",
        "    \n",
        "    y_pred_norm = model.predict([sample_X_enc, sample_X_dec], verbose=0)\n",
        "    \n",
        "    cell_id = cell_ids_test[index]\n",
        "    scaler = scalers[cell_id]\n",
        "\n",
        "    SEQ_LEN_OUT = y_true_norm.shape[1]\n",
        "    n_cols = 5\n",
        "    n_rows = (SEQ_LEN_OUT + n_cols - 1) // n_cols\n",
        "\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 4 * n_rows))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    titolo = f'Previsione Multi-Step per il Campione di Test {index} (Cella: {cell_id})'\n",
        "    fig.suptitle(titolo, fontsize=16, y=1.02)\n",
        "\n",
        "    for t in range(SEQ_LEN_OUT):\n",
        "        ax = axes[t]\n",
        "\n",
        "        true_sample_norm = y_true_norm[index, t, :]\n",
        "        pred_sample_norm = y_pred_norm[0, t, :]\n",
        "        \n",
        "        true_sample_denorm = scaler.inverse_transform(true_sample_norm.reshape(1, -1)).flatten()\n",
        "        pred_sample_denorm = scaler.inverse_transform(pred_sample_norm.reshape(1, -1)).flatten()\n",
        "        \n",
        "        # Indici reali e immaginari\n",
        "        r_idx = list(range(0, 118, 2))\n",
        "        i_idx = list(range(1, 118, 2))\n",
        "\n",
        "        # Parte immaginaria negativa (standard per Nyquist)\n",
        "        df_true = pd.DataFrame({'real': true_sample_denorm[r_idx], 'imag': true_sample_denorm[i_idx]})\n",
        "        df_pred = pd.DataFrame({'real': pred_sample_denorm[r_idx], 'imag': pred_sample_denorm[i_idx]})\n",
        "\n",
        "        \n",
        "        # Ultime due feature: temperatura e SOH\n",
        "        soh_true = true_sample_denorm[-1]\n",
        "        soh_pred = pred_sample_denorm[-1]\n",
        "        \n",
        "        soh_true = np.round(soh_true, 0)\n",
        "        soh_pred = np.round(soh_pred, 0)\n",
        "\n",
        "        ax.plot(df_true['real'], df_true['imag'], 'o-', label='Reale', color='blue', markersize=4)\n",
        "        ax.plot(df_pred['real'], df_pred['imag'], 'x--', label='Predetto', color='red', markersize=4)\n",
        "\n",
        "        ax.set_xlabel('Re(Z) [Î©]')\n",
        "        ax.set_ylabel('âˆ’Im(Z) [Î©]')\n",
        "        ax.legend()\n",
        "\n",
        "        title_text = (f'Timestep Futuro: +{t+1}\\n'\n",
        "                      f'SOH Reale: {soh_true:.3f} (Pred: {soh_pred:.3f})')\n",
        "        ax.set_title(title_text, fontsize=9)\n",
        "        ax.grid(True)\n",
        "        ax.axis(\"equal\")\n",
        "\n",
        "    # Disattiva assi inutilizzati\n",
        "    for i in range(SEQ_LEN_OUT, len(axes)):\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33bbe405",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model_on_full_testset(model, X_test_enc, X_test_dec, y_test_norm, scaler):\n",
        "    y_pred_norm = model.predict([X_test_enc, X_test_dec])\n",
        "\n",
        "    nsamples, ntimesteps, nfeatures = y_test_norm.shape\n",
        "\n",
        "    y_test_denorm = scaler.inverse_transform(y_test_norm.reshape(-1, nfeatures)).reshape(nsamples, ntimesteps, nfeatures)\n",
        "    y_pred_denorm = scaler.inverse_transform(y_pred_norm.reshape(-1, nfeatures)).reshape(nsamples, ntimesteps, nfeatures)\n",
        "\n",
        "    #temp_true = y_test_denorm[:, :, -2].flatten()\n",
        "    #temp_pred = y_pred_denorm[:, :, -2].flatten()\n",
        "    soh_true = y_test_denorm[:, :, -1].flatten()\n",
        "    soh_pred = y_pred_denorm[:, :, -1].flatten()\n",
        "    \n",
        "    #temp_true = np.round(temp_true, 0)\n",
        "    #temp_pred = np.round(temp_pred, 0)\n",
        "    \n",
        "    soh_true = np.round(soh_true, 0)\n",
        "    soh_pred = np.round(soh_pred, 0)\n",
        "\n",
        "    ridx = list(range(0, nfeatures-2, 2))\n",
        "    iidx = list(range(1, nfeatures-2, 2))\n",
        "    re_true = y_test_denorm[:, :, ridx].flatten()\n",
        "    re_pred = y_pred_denorm[:, :, ridx].flatten()\n",
        "    im_true = y_test_denorm[:, :, iidx].flatten()\n",
        "    im_pred = y_pred_denorm[:, :, iidx].flatten()\n",
        "\n",
        "    metrics = {\n",
        "        #\"temperature\": {\"r2\": r2_score(temp_true, temp_pred), \"mse\": mean_squared_error(temp_true, temp_pred), \"mae\": mean_absolute_error(temp_true, temp_pred)},\n",
        "        \"soh\": {\"r2\": r2_score(soh_true, soh_pred), \"mse\": mean_squared_error(soh_true, soh_pred), \"mae\": mean_absolute_error(soh_true, soh_pred)},\n",
        "        \"impedance_real\": {\"r2\": r2_score(re_true, re_pred), \"mse\": mean_squared_error(re_true, re_pred), \"mae\": mean_absolute_error(re_true, re_pred)},\n",
        "        \"impedance_imag\": {\"r2\": r2_score(im_true, im_pred), \"mse\": mean_squared_error(im_true, im_pred), \"mae\": mean_absolute_error(im_true, im_pred)}\n",
        "    }\n",
        "    \n",
        "    print(\"--- Metriche Aggregate sul Test Set (con Scaler Globale) ---\")\n",
        "    for key, vals in metrics.items():\n",
        "        print(f\"{key}: R2={vals['r2']:.4f}, MSE={vals['mse']:.4f}, MAE={vals['mae']:.4f}\")\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41bc43a2",
      "metadata": {
        "id": "41bc43a2"
      },
      "outputs": [],
      "source": [
        "def evaluate_model_per_cell(model, X_test_enc, X_test_dec, y_test, scalers, cell_ids_test, cell_id_to_evaluate):\n",
        "    \"\"\"\n",
        "    Valuta le performance del modello sui dati di test di una specifica cella,\n",
        "    usando lo scaler addestrato solo su quella cella.\n",
        "\n",
        "    Args:\n",
        "        model (tf.keras.Model): Il modello addestrato.\n",
        "        X_test_enc, X_test_dec, y_test: L'intero set di test.\n",
        "        scalers (dict): Dizionario contenente gli scaler per ogni cella.\n",
        "        cell_ids_test (np.ndarray): Array con gli ID delle celle per ogni campione di test.\n",
        "        cell_id_to_evaluate (str): L'ID della cella da valutare (es. 'cell_1').\n",
        "    \"\"\"\n",
        "    print(f\"--- Inizio Valutazione Specifica per '{cell_id_to_evaluate}' ---\")\n",
        "    \n",
        "    indices = np.where(cell_ids_test == cell_id_to_evaluate)[0]\n",
        "    if len(indices) == 0:\n",
        "        print(f\"Nessun campione di test trovato per '{cell_id_to_evaluate}'.\")\n",
        "        return None\n",
        "\n",
        "    X_enc_cell = X_test_enc[indices]\n",
        "    X_dec_cell = X_test_dec[indices]\n",
        "    y_true_norm = y_test[indices]\n",
        "\n",
        "    try:\n",
        "        scaler = scalers[cell_id_to_evaluate]\n",
        "    except KeyError:\n",
        "        print(f\"Errore: Scaler per '{cell_id_to_evaluate}' non trovato.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Esecuzione previsioni su {len(X_enc_cell)} campioni per '{cell_id_to_evaluate}'...\")\n",
        "    y_pred_norm = model.predict([X_enc_cell, X_dec_cell], batch_size=BATCH_SIZE, verbose=0)\n",
        "    \n",
        "    n_samples, n_timesteps, n_features = y_true_norm.shape\n",
        "    y_true_denorm = scaler.inverse_transform(y_true_norm.reshape(-1, n_features)).reshape(n_samples, n_timesteps, n_features)\n",
        "    y_pred_denorm = scaler.inverse_transform(y_pred_norm.reshape(-1, n_features)).reshape(n_samples, n_timesteps, n_features)\n",
        "\n",
        "    #temp_true = y_true_denorm[:, :, -2].flatten()\n",
        "    #temp_pred = y_pred_denorm[:, :, -2].flatten()\n",
        "    soh_true = y_true_denorm[:, :, -1].flatten()\n",
        "    soh_pred = y_pred_denorm[:, :, -1].flatten()\n",
        "    \n",
        "    #temp_true = np.round(temp_true, 0)\n",
        "    #temp_pred = np.round(temp_pred, 0)\n",
        "    \n",
        "    soh_true = np.round(soh_true, 0)\n",
        "    soh_pred = np.round(soh_pred, 0)\n",
        "    \n",
        "    ridx = list(range(0, n_features-2, 2))\n",
        "    iidx = list(range(1, n_features-2, 2))\n",
        "    re_true = y_true_denorm[:, :, ridx].flatten()\n",
        "    re_pred = y_pred_denorm[:, :, ridx].flatten()\n",
        "    im_true = y_true_denorm[:, :, iidx].flatten()\n",
        "    im_pred = y_pred_denorm[:, :, iidx].flatten()\n",
        "\n",
        "    metrics = {\n",
        "        #\"temperature\": {\"r2\": r2_score(temp_true, temp_pred), \"mse\": mean_squared_error(temp_true, temp_pred), \"mae\": mean_absolute_error(temp_true, temp_pred)},\n",
        "        \"soh\": {\"r2\": r2_score(soh_true, soh_pred), \"mse\": mean_squared_error(soh_true, soh_pred), \"mae\": mean_absolute_error(soh_true, soh_pred)},\n",
        "        \"impedance_real\": {\"r2\": r2_score(re_true, re_pred), \"mse\": mean_squared_error(re_true, re_pred), \"mae\": mean_absolute_error(re_true, re_pred)},\n",
        "        \"impedance_imag\": {\"r2\": r2_score(im_true, im_pred), \"mse\": mean_squared_error(im_true, im_pred), \"mae\": mean_absolute_error(im_true, im_pred)}\n",
        "    }\n",
        "    \n",
        "    print(f\"\\n--- RISULTATI PER '{cell_id_to_evaluate}' ---\")\n",
        "    for key, vals in metrics.items():\n",
        "        print(f\"  {key:<15}: R2={vals['r2']:.4f}, MSE={vals['mse']:.4f}, MAE={vals['mae']:.4f}\")\n",
        "    \n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80b5cbeb",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_lstm_seq2seq_model(seq_len_in, num_features_in, seq_len_out, num_features_out, \n",
        "                                latent_dim=64, num_encoder_layers=3, dropout=0.2):\n",
        "    \"\"\"\n",
        "    Crea un modello LSTM Encoder-Decoder (Seq2Seq) usando l'API Funzionale di Keras.\n",
        "    \"\"\"\n",
        "    # --- ENCODER ---\n",
        "    encoder_inputs = layers.Input(shape=(None, num_features_in), name='encoder_input')\n",
        "    \n",
        "    encoder_lstm_outputs = encoder_inputs\n",
        "    for i in range(num_encoder_layers - 1):\n",
        "        encoder_lstm_outputs = layers.LSTM(latent_dim, return_sequences=True)(encoder_lstm_outputs)\n",
        "        encoder_lstm_outputs = layers.Dropout(dropout)(encoder_lstm_outputs)\n",
        "\n",
        "    _, state_h, state_c = layers.LSTM(latent_dim, return_state=True, name='encoder_lstm_last')(encoder_lstm_outputs)\n",
        "    encoder_states = [state_h, state_c] # Questo Ã¨ il \"vettore di contesto\".\n",
        "\n",
        "    # --- DECODER ---\n",
        "    decoder_inputs = layers.Input(shape=(None, num_features_out), name='decoder_input')\n",
        "    \n",
        "    decoder_lstm = layers.LSTM(latent_dim, return_sequences=True, name='decoder_lstm')\n",
        "    decoder_outputs = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "    decoder_outputs = layers.Dropout(dropout)(decoder_outputs)\n",
        "\n",
        "    # --- OUTPUT LAYER ---\n",
        "    output_projection = layers.Dense(num_features_out, name='output_projection')\n",
        "    final_outputs = layers.TimeDistributed(output_projection)(decoder_outputs)\n",
        "    \n",
        "    model = tf.keras.Model([encoder_inputs, decoder_inputs], final_outputs, name='lstm_seq2seq')\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "041ea940-58a1-4300-96cd-51c92a17b29b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "041ea940-58a1-4300-96cd-51c92a17b29b",
        "outputId": "d7f5ed40-6fd7-4b5a-f86f-cff25c1b5f55"
      },
      "outputs": [],
      "source": [
        "X_test_enc[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "016cd809-f935-463a-9c75-8dd708a6a907",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "016cd809-f935-463a-9c75-8dd708a6a907",
        "outputId": "2749c0b4-8341-4987-b64d-e6a3d36a633b"
      },
      "outputs": [],
      "source": [
        "y_test[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c94d71f4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c94d71f4",
        "outputId": "33628db3-1690-4cf4-9710-c04d5a683dbb"
      },
      "outputs": [],
      "source": [
        "y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c66ce2e",
      "metadata": {},
      "outputs": [],
      "source": [
        "y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b62f44a",
      "metadata": {
        "id": "4b62f44a"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "205a5966-f0c2-4825-948c-2496471a4dd4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "205a5966-f0c2-4825-948c-2496471a4dd4",
        "outputId": "fa789e1c-d3cf-4fa6-df32-c45617d43170",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "model = EncoderDecoderTransformer(\n",
        "    seq_len_in=SEQ_LEN_IN,\n",
        "    seq_len_out=SEQ_LEN_OUT,\n",
        "    num_features_in=NUM_FEATURES_IN,\n",
        "    num_features_out=NUM_FEATURES_OUT,\n",
        "    d_model=64,\n",
        "    num_heads=6,\n",
        "    ff_dim=64,\n",
        "    num_encoder_layers=3,\n",
        "    num_decoder_layers=3,\n",
        "    dropout=0.2\n",
        ")\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.AdamW(learning_rate=1e-4, weight_decay=1e-4), loss=combined_loss)\n",
        "\n",
        "\n",
        "# L'input Ã¨ una lista di due tensori: [encoder_input, decoder_input]\n",
        "history = model.fit(\n",
        "    [X_train_enc, X_train_dec], y_train,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_data=([X_val_enc, X_val_dec], y_val),\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=PATIENCE,\n",
        "            restore_best_weights=True,\n",
        "            start_from_epoch=5,\n",
        "            min_delta=0.005\n",
        "        ),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=PATIENCE//4,\n",
        "            min_lr=1e-7,\n",
        "            verbose=1,\n",
        "            min_delta=0.01\n",
        "        )\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c15a98cc-3a0c-4b7b-80fb-cac38107044e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "c15a98cc-3a0c-4b7b-80fb-cac38107044e",
        "outputId": "ceb3b16b-9f95-4ef4-8a64-5a508696c362"
      },
      "outputs": [],
      "source": [
        "scaler_global = load(\"scaler_global.joblib\")\n",
        "\n",
        "metrics_global = evaluate_model_on_full_testset(\n",
        "    model,\n",
        "    X_test_enc, \n",
        "    X_test_dec, \n",
        "    y_test, \n",
        "    scaler=scaler_global\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ErXsnkjawLbV",
      "metadata": {
        "id": "ErXsnkjawLbV"
      },
      "outputs": [],
      "source": [
        "model.save(\"new_model.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3xnC1RIyb2Y2",
      "metadata": {
        "id": "3xnC1RIyb2Y2"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.load_model(\n",
        "    \"new_model.keras\",\n",
        "    custom_objects={\n",
        "        'combined_loss': combined_loss\n",
        "    }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad360ad4",
      "metadata": {},
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76e8a9e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "scalers_per_cell = load(\"scalers_per_cell.joblib\")\n",
        "\n",
        "unique_cells_in_test = np.unique(cell_ids_test)\n",
        "all_metrics_per_cell = {}\n",
        "\n",
        "for cell_id in unique_cells_in_test:\n",
        "    metrics = evaluate_model_per_cell(\n",
        "        model,\n",
        "        X_test_enc,\n",
        "        X_test_dec,\n",
        "        y_test,\n",
        "        scalers=scalers_per_cell,\n",
        "        cell_ids_test=cell_ids_test,\n",
        "        cell_id_to_evaluate=cell_id\n",
        "    )\n",
        "    if metrics:\n",
        "        all_metrics_per_cell[cell_id] = metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52c20a8e",
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler_global = load(\"scaler_global.joblib\")\n",
        "\n",
        "metrics = evaluate_model_on_full_testset(\n",
        "    model,\n",
        "    X_test_enc, \n",
        "    X_test_dec, \n",
        "    y_test, \n",
        "    scaler_global\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd5e85aa",
      "metadata": {},
      "outputs": [],
      "source": [
        "scalers_per_cell = load(\"scalers_per_cell.joblib\")\n",
        "\n",
        "print_eis_predictions(\n",
        "    model=model,\n",
        "    X_test_enc=X_test_enc,\n",
        "    X_test_dec=X_test_dec,\n",
        "    y_true_norm=y_test,\n",
        "    scalers=scalers_per_cell,\n",
        "    cell_ids_test=cell_ids_test, \n",
        "    index=100\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "effca8de",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = EncoderDecoderTransformer(\n",
        "    seq_len_in=SEQ_LEN_IN,\n",
        "    seq_len_out=SEQ_LEN_OUT,\n",
        "    num_features_in=NUM_FEATURES_IN,\n",
        "    num_features_out=NUM_FEATURES_OUT,\n",
        "    d_model=64,\n",
        "    num_heads=6,\n",
        "    ff_dim=64,\n",
        "    num_encoder_layers=3,\n",
        "    num_decoder_layers=3,\n",
        "    dropout=0.2\n",
        ")\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.AdamW(learning_rate=1e-4, weight_decay=1e-4), loss=\"mean_squared_error\")\n",
        "\n",
        "\n",
        "# L'input Ã¨ una lista di due tensori: [encoder_input, decoder_input]\n",
        "history = model.fit(\n",
        "    [X_train_enc, X_train_dec], y_train,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_data=([X_val_enc, X_val_dec], y_val),\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=PATIENCE,\n",
        "            restore_best_weights=True,\n",
        "            start_from_epoch=5,\n",
        "            min_delta=0.0005\n",
        "        ),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=PATIENCE//4,\n",
        "            min_lr=1e-7,\n",
        "            verbose=1,\n",
        "            min_delta=0.01\n",
        "        )\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d90daf8e",
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save(\"model_mse_multi_cell.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14a75b23",
      "metadata": {},
      "outputs": [],
      "source": [
        "scalers_per_cell = load(\"scalers_per_cell.joblib\")\n",
        "\n",
        "unique_cells_in_test = np.unique(cell_ids_test)\n",
        "all_metrics_per_cell = {}\n",
        "\n",
        "for cell_id in unique_cells_in_test:\n",
        "    metrics = evaluate_model_per_cell(\n",
        "        model,\n",
        "        X_test_enc,\n",
        "        X_test_dec,\n",
        "        y_test,\n",
        "        scalers=scalers_per_cell,\n",
        "        cell_ids_test=cell_ids_test,\n",
        "        cell_id_to_evaluate=cell_id\n",
        "    )\n",
        "    if metrics:\n",
        "        all_metrics_per_cell[cell_id] = metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11ac0588",
      "metadata": {},
      "outputs": [],
      "source": [
        "scalers_per_cell = load(\"scalers_per_cell.joblib\")\n",
        "\n",
        "print_eis_predictions(\n",
        "    model=model,\n",
        "    X_test_enc=X_test_enc,\n",
        "    X_test_dec=X_test_dec,\n",
        "    y_true_norm=y_test,\n",
        "    scalers=scalers_per_cell,\n",
        "    cell_ids_test=cell_ids_test, \n",
        "    index=100\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d28f2e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = tf.keras.models.load_model(\n",
        "    \"model_mse_multi_cell.keras\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e1dfb3b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def autoregressive_forecast(model, X_enc, seq_len_out, start_token=None):\n",
        "    \"\"\"\n",
        "    Genera la sequenza di output in modo autoregressivo (senza teacher forcing).\n",
        "    \"\"\"\n",
        "    n_features = X_enc.shape[-1]\n",
        "    if start_token is None:\n",
        "        start_token = np.zeros((n_features,), dtype=np.float32)\n",
        "\n",
        "    # inizializza decoder con solo il token iniziale\n",
        "    y_dec = np.repeat(start_token[None, None, :], X_enc.shape[0], axis=0)  # (B,1,F)\n",
        "\n",
        "    preds = []\n",
        "    for _ in range(seq_len_out):\n",
        "        # predizione step corrente\n",
        "        y_pred = model.predict([X_enc, y_dec], verbose=0)[:, -1:, :]  # ultimo timestep\n",
        "        preds.append(y_pred)\n",
        "        # aggiungi la predizione corrente al decoder\n",
        "        y_dec = np.concatenate([y_dec, y_pred], axis=1)\n",
        "\n",
        "    return np.concatenate(preds, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4bac21d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "def autoregressive_forecast(model, X_enc, y_true, start_token=None, verbose=True, plot_steps=False):\n",
        "    \"\"\"\n",
        "    Genera previsioni autoregressive step-by-step e calcola MSE e MAE globali.\n",
        "    Se plot_steps=True e batch_size=1, mostra l'evoluzione della previsione a ogni step.\n",
        "    \"\"\"\n",
        "    batch_size = X_enc.shape[0]\n",
        "    seq_len_out = y_true.shape[1]\n",
        "    n_features = y_true.shape[2]\n",
        "\n",
        "    if start_token is None:\n",
        "        start_token = np.zeros((n_features,), dtype=np.float32)\n",
        "\n",
        "    y_dec = np.repeat(start_token[None, None, :], batch_size, axis=0)\n",
        "    preds = []\n",
        "\n",
        "    for step in range(seq_len_out):\n",
        "        y_pred = model.predict([X_enc, y_dec], verbose=0)[:, -1:, :]\n",
        "        preds.append(y_pred)\n",
        "        y_dec = np.concatenate([y_dec, y_pred], axis=1)\n",
        "\n",
        "        # --- visualizza progressivo se richiesto ---\n",
        "        if plot_steps and batch_size == 1:\n",
        "            plt.figure(figsize=(5,5))\n",
        "            r_idx = list(range(0, n_features-2, 2))\n",
        "            i_idx = [i+1 for i in r_idx]\n",
        "            real_true = y_true[0, :, r_idx].flatten()\n",
        "            imag_true = y_true[0, :, i_idx].flatten()\n",
        "            real_pred = np.concatenate(preds, axis=1)[0, :, r_idx].flatten()\n",
        "            imag_pred = np.concatenate(preds, axis=1)[0, :, i_idx].flatten()\n",
        "            plt.plot(real_true, -imag_true, 'o-', label='True')\n",
        "            plt.plot(real_pred, -imag_pred, 's--', label=f'Pred @ step {step+1}')\n",
        "            plt.title(f'Autoregressive step {step+1}/{seq_len_out}')\n",
        "            plt.xlabel('Zreal [Î©]')\n",
        "            plt.ylabel('-Zimag [Î©]')\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "    # --- unisci tutte le predizioni finali ---\n",
        "    y_pred = np.concatenate(preds, axis=1)\n",
        "\n",
        "    # metriche globali\n",
        "    mse = mean_squared_error(y_true.reshape(-1), y_pred.reshape(-1))\n",
        "    mae = mean_absolute_error(y_true.reshape(-1), y_pred.reshape(-1))\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"ðŸ” Autoregressive forecast done for {seq_len_out} steps\")\n",
        "        print(f\"ðŸ“Š MSE: {mse:.6f} | MAE: {mae:.6f}\")\n",
        "\n",
        "    return y_pred, mse, mae\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7b0202a",
      "metadata": {},
      "outputs": [],
      "source": [
        "index = 0  # scegli il campione\n",
        "x_enc_sample = X_test_enc[index:index+1]\n",
        "y_true_sample = y_test[index:index+1]\n",
        "\n",
        "# predizione con visualizzazione step-by-step\n",
        "y_pred_sample, mse_sample, mae_sample = autoregressive_forecast(\n",
        "    model,\n",
        "    x_enc_sample,\n",
        "    y_true_sample,\n",
        "    plot_steps=True\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8062e36c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "def autoregressive_forecast(model, X_enc, y_true, start_token=None, verbose=True, plot_steps=False):\n",
        "    \"\"\"\n",
        "    Genera previsioni autoregressive step-by-step e calcola MSE e MAE globali.\n",
        "    Se plot_steps=True e batch_size=1, mostra l'evoluzione della previsione a ogni step.\n",
        "    \n",
        "    Args:\n",
        "        model (tf.keras.Model): Il modello Transformer.\n",
        "        X_enc (np.array): Input per l'Encoder (sequenza storica).\n",
        "        y_true (np.array): Output target REALE (usato solo per metriche/plot TRUE).\n",
        "        start_token (np.array, optional): Il primo token per inizializzare il decoder.\n",
        "        verbose (bool): Stampa le metriche finali.\n",
        "        plot_steps (bool): Plotta la curva di Nyquist a ogni timestep (solo per batch_size=1).\n",
        "\n",
        "    Returns:\n",
        "        tuple: (y_pred, mse, mae)\n",
        "    \"\"\"\n",
        "    batch_size = X_enc.shape[0]\n",
        "    seq_len_out = y_true.shape[1]\n",
        "    n_features = y_true.shape[2]\n",
        "    \n",
        "    # Calcola il numero di feature EIS (tutte tranne le ultime due: SOH e Temperatura)\n",
        "    eis_features_count = n_features - 2\n",
        "    \n",
        "    if start_token is None:\n",
        "        start_token = np.zeros((n_features,), dtype=np.float32)\n",
        "\n",
        "    y_dec = np.repeat(start_token[None, None, :], batch_size, axis=0) # (B, 1, F)\n",
        "    preds = []\n",
        "\n",
        "    for step in range(seq_len_out):\n",
        "        # 1. PREDIZIONE: Prendi solo l'ultimo elemento predetto (il nuovo)\n",
        "        y_pred = model.predict([X_enc, y_dec], verbose=0)[:, -1:, :]\n",
        "        preds.append(y_pred)\n",
        "        \n",
        "        # 2. AUTOREGRESSIONE: Aggiungi la predizione all'input per il prossimo step\n",
        "        y_dec = np.concatenate([y_dec, y_pred], axis=1)\n",
        "\n",
        "        # Sostituisci il blocco 'if plot_steps...' con questo codice aggiornato\n",
        "\n",
        "        # --- visualizza progressivo se richiesto ---\n",
        "        if plot_steps and batch_size == 1:\n",
        "            plt.figure(figsize=(5,5))\n",
        "            \n",
        "            # Indici Reale e Immaginario per le feature EIS\n",
        "            # eis_features_count = n_features - 2 (Ã¨ giÃ  stato calcolato prima)\n",
        "            r_idx = list(range(0, eis_features_count, 2))\n",
        "            i_idx = list(range(1, eis_features_count, 2))\n",
        "            \n",
        "            # Dati Reali (completi)\n",
        "            real_true = y_true[0, :, r_idx].flatten()\n",
        "            imag_true = y_true[0, :, i_idx].flatten()\n",
        "            \n",
        "            # TRONCAMENTO: Assicurati che real_true abbia la stessa lunghezza di imag_true\n",
        "            if len(real_true) > len(imag_true):\n",
        "                real_true = real_true[:len(imag_true)]\n",
        "            \n",
        "            # Dati Predetti (fino allo step corrente)\n",
        "            current_preds_seq = np.concatenate(preds, axis=1)\n",
        "            real_pred = current_preds_seq[0, :, r_idx].flatten()\n",
        "            imag_pred = current_preds_seq[0, :, i_idx].flatten()\n",
        "            \n",
        "            # TRONCAMENTO: Assicurati che real_pred abbia la stessa lunghezza di imag_pred\n",
        "            if len(real_pred) > len(imag_pred):\n",
        "                real_pred = real_pred[:len(imag_pred)]\n",
        "            \n",
        "            plt.plot(real_true, -imag_true, 'o-', label='Reale Completa', color='blue')\n",
        "            plt.plot(real_pred, -imag_pred, 's--', label=f'Predetta (fino a t={step+1})', color='red')\n",
        "            \n",
        "            plt.title(f'Autoregressive Nyquist Step {step+1}/{seq_len_out}')\n",
        "            plt.xlabel('Re(Z) [Î©]')\n",
        "            plt.ylabel('âˆ’Im(Z) [Î©]')\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "            plt.axis('equal')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "    # --- unisci tutte le predizioni finali ---\n",
        "    y_pred = np.concatenate(preds, axis=1)\n",
        "\n",
        "    # metriche globali su tutte le feature (EIS, SOH, Temp)\n",
        "    mse = mean_squared_error(y_true.reshape(-1), y_pred.reshape(-1))\n",
        "    mae = mean_absolute_error(y_true.reshape(-1), y_pred.reshape(-1))\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"ðŸ” Autoregressive forecast done for {seq_len_out} steps\")\n",
        "        print(f\"ðŸ“Š MSE: {mse:.6f} | MAE: {mae:.6f}\")\n",
        "\n",
        "    return y_pred, mse, mae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a2fca5e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Scegli un indice a caso (e.g., il 100Â° campione)\n",
        "index = 100 \n",
        "\n",
        "# 2. Estrai il singolo campione ENCODER (X_enc) e il target (y_true)\n",
        "# Usiamo lo slicing [index:index+1] per mantenere la dimensione del batch (1, SEQ_LEN, NUM_FEATURES)\n",
        "x_enc_sample = X_test_enc[index:index+1]\n",
        "y_true_sample = y_test[index:index+1]\n",
        "\n",
        "# 3. Esegui la predizione con la visualizzazione step-by-step\n",
        "y_pred_sample, mse_sample, mae_sample = autoregressive_forecast(\n",
        "    model,\n",
        "    x_enc_sample,\n",
        "    y_true_sample,\n",
        "    plot_steps=True # Attiva la visualizzazione dell'evoluzione del Nyquist step-by-step\n",
        ")\n",
        "\n",
        "# 4. Stampa le metriche per quel singolo campione\n",
        "print(f\"\\n--- Risultati Predizione Autoregressiva sul Campione {index} ---\")\n",
        "print(f\"MSE sul campione: {mse_sample:.6f}\")\n",
        "print(f\"MAE sul campione: {mae_sample:.6f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
